%!TEX root = ../main.tex
\section{Productivity of Cofunctions}
\label{sec:productivity}

Before delving into a description of our proposal for a productivity checking algorithm for definitions with copatterns, we will discuss the properties of other approaches, and why they may or may not be desirable.

\subsection{Lazy Evaluation: The Haskell Approach}
It may be compelling to think that we can simply model coinductive data in the same way as it is done in Haskell, by evaluating all expressions lazily. Then any productivity algorithm would work for Haskell as well, making the result more generally applicable. The lazy evaluation approach, however, is insufficient in several aspects.

First and foremost, if we insist on making no distiction between inductive and coinductive data in our type system (Haskell makes no distinction), subject reduction is lost in a dependently typed system such as Idris\,\citep{Abel13Copatterns}. While we will not unfold the argument in its entirety here, the problem lies in the fact that coinductive data is modeled as the \emph{construction} of infinite trees, making dependent pattern matching possible on codata, rather than in terms of its \emph{destructors}. Naturally, we want to preserve subject reduction in the Idris type system.

Secondly, Idris has the Church-Rosser property, meaning that distinct reduction strategies lead to the same normal form\,\citep{BradyIdrisImpl13}. Furthermore, the total part of Idris is strongly normalizing, such that it enjoys the \emph{strong} Church-Rosser property\,\citep{Turner04totalfunctional}. This means that not only will every reduction strategy leading to a normal form lead to the same normal form, but \emph{any} reduction strategy must lead to a normal form. In Haskell, many definitions will lead to a normal form under lazy evaluation, while eager evaluation would lead to infinite recursion. Therefore, exploiting lazy evaluation in the total part of Idris would mean that it would no longer have the strong Church-Rosser property.

\subsection{Productivity Checking with Sized Types}
As shown by Abel and Pientka\,\citep{Abel13Wellfounded}, verifying the productivity of coinductive definitions with copatterns is indeed possible using sized types. Thus, the main argument against using sized types is one of usability, from the point of view of the Idris user. Thibodeau\,\citep{Thibodeau11} emphasizes that size annotations generally make the code harder to read and seem unnecessary (see Section~\ref{sec:related_work}). In our own experience, size annotations do place quite a burden of bookkeeping upon the user. While having more expressive types is generally considered a value tool for building correct programs, sized types do have a tendency to become something you use in order to satisfy the productivity checker, rather than because it leads to clearer type specifications. Because totality is optional in Idris (as opposed to in Agda, for instance), this means that in a practical programming setting, some users would most likely be inclined to switch off productivity checking for coinductive definitions, or refrain from using them entirely.

This argument is only valid as long as we have no way to fully reconstruct all size annotations. Once we do (if ever), the burden of sized types can be placed entirely upon the compiler. At such point, termination checking with sized types would seem like a fine approach.

\subsection{Guarded Corecursion}
In its original form presented by Coquand\,\citep{Coquand94} and Gim\'{e}nez\,\citep{Gimenez95}, the guardedness condition is generally too restrictive, as exemplified in Section~\ref{sec:related_work}. The work done on the ESFP system by Telford and Turner\,\citep{Telford97ensuringstreams,Telford98ensuringthe} makes the guardedness criterion more generally applicable, even though some problems still remain, such as handling indirect application to corecursive functions. An example of this problem is the \texttt{g} function in Figure~\ref{fig:TelfordTurnerProblems}, whose guardedness cannot be determined due to the indirect call to \texttt{id}. The function \texttt{f} is not considered guarded within their system because forward references may not be made to the rest of the process, i.e. the head of \texttt{f} cannot refer to the tail of \texttt{f}.

\begin{figure}
\begin{alltt}
f = (head (tail f)) :: (1 :: f)                 g = 1 :: ((fst funPair) g)
                                                where funPair = (id, id)
\end{alltt}
\caption{Two types of functions not deemed productive by the extended guardedness criterion by Telford and Turner\,\citep[Section6.3]{Telford98ensuringthe}. Here, \texttt{::} is the cons operator for a hypothetical definition of streams without copatterns, and \texttt{id} is the identity function. The definitions of \texttt{head} and \texttt{tail} are as one would expect.} 
\label{fig:TelfordTurnerProblems}
\end{figure}

Abel\,\citep{Abel99terminationchecking} argues that termination analysis should abstract from the syntactic structure of the program being analyzed, since this makes the outcome of the analysis prone to small changes in program structure. While this argument seems sensible, the advantage of a syntactic check is that productivity does not depend on the user's ability or willingness to apply the right size annotations in the right places, as opposed to what is currently the case for sized types.

\paragraph{}
What we ultimately seek is a productivity checking algorithm for definitions with copatterns that covers as many cases as possible, without burdening the user too much. In the following, we will provide a detailed description of our proposal.
%An explanation of our proposal for productivity checking in collaboration with size-change termination.
% Why not guardedness?
% Why not sized types? 
% Why not the haskell way?
\subsection{Proposed Solution}
The basic idea of our proposal is \textit{time measurement}. We wish to always know how many observations we can safely make on a given recursive reference. To do this we use time as an analogy. To define what happens at a given time we can only refer to what has happened earlier. Each observation in a copattern says what happens one time unit later. For this we will exemplify using days. If a given function \texttt{f} ``happens'' on day $\tau$, then the first observation on \texttt{f} happens on day $\tau+1$, the second observation on day $\tau+2$ etc. If we wish to define what happens on any given day $\tau+n$ we can only refer to things happening earlier, on day $D$ where $\tau+n\, \textgreater \,D$. Any definition that comply with this is said to be productive.

In Figure~\ref{fig:zeros} we decorate a definition \texttt{zeros} with these time measures. Since both the \texttt{head} and the \texttt{tail} cases are one observation deep, they both get time measure $\tau+1$. Now we must look on the right hand sides of these observations. The \texttt{head} observation is simple as there are no recursive calls. This gets the time measure $always$ as we assume \texttt{Z} is always known. In the \texttt{tail} observation we have a recursive reference to \texttt{zeros}. This gets the time measure $\tau$ which is the time of \texttt{zeros}. We can say that \texttt{zeros} is productive because in the \texttt{head} case $\tau+1\,\textgreater\,always$ and in the \texttt{tail} $\tau+1\,\textgreater\,\tau$. Note that all these annotations can be inferred, and does not need the user to specify them.

\begin{figure}
\begin{tabular}{l c}
\begin{minipage}{3in}
\begin{Verbatim}[commandchars=\\\{\},codes={\catcode`$=3\catcode`_=8}]
zeros : Stream Nat
head zeros = Z
tail zeros = zeros
\end{Verbatim}
\end{minipage} &
\begin{minipage}{3in}
\begin{Verbatim}[commandchars=\\\{\},codes={\catcode`$=3\catcode`_=8}]
zeros$_{\tau}$ : Stream Nat
head$_{\tau+1}$ zeros$_{\tau}$ = Z$_{always}$
tail$_{\tau+1}$ zeros$_{\tau}$ = zeros$_{\tau}$
\end{Verbatim}
\end{minipage}
\end{tabular}
\caption{An infinite stream of zeros. To the left is the implementation, and to the right is the implementation decorated with depth measures.}
\label{fig:zeros}
\end{figure}

In Figure~\ref{fig:zerosprime} we see a faulty implementation of \texttt{zeros} called \texttt{zeros'}. This is not a productive implementation. We decorate the program in the same fashion as above, but this time the recursive reference in the \texttt{tail} case has time measure $\tau+1$ rather than $\tau$ because we make a right hand side observation on it. For this to be productive $\tau+1\,\textgreater\,\tau+1$ which it clearly is not. Therefore we can say that \texttt{zeros'} is not productive.

\begin{figure}
\begin{tabular}{l c}
\begin{minipage}{3in}
\begin{Verbatim}[commandchars=\\\{\},codes={\catcode`$=3\catcode`_=8}]
zeros' : Stream Nat
head zeros' = Z
tail zeros' = tail zeros'
\end{Verbatim}
\end{minipage} &
\begin{minipage}{3in}
\begin{Verbatim}[commandchars=\\\{\},codes={\catcode`$=3\catcode`_=8}]
zeros'$_{\tau}$ : Stream Nat
head$_{\tau+1}$ zeros'$_{\tau}$ = Z$_{\tau-1}$
tail$_{\tau+1}$ zeros'$_{\tau}$ = tail$_{\tau+1}$ zeros'$_{\tau}$
\end{Verbatim}
\end{minipage}
\end{tabular}
\caption{A non-productive implementation of zeros.}
\label{fig:zerosprime}
\end{figure}

\subsubsection{Coinductive Definitions with Coinductive Parameters}

Sometimes we want to write definitions that depend on other functions with coinductive parameters. When decorating \texttt{nats} in Figure~\ref{fig:nats} we can not give the right hand side of the \texttt{tail} observation an accurate time measure as we don't know what \texttt{map} does to \texttt{nats}. We must first analyse \texttt{map} to see the effect it has on it parameters.

\begin{figure}
\begin{Verbatim}[commandchars=\\\{\},codes={\catcode`$=3\catcode`_=8}]
nats$_{\tau}$ : Stream Nat
head$_{\tau+1}$ zeros$_{\tau}$ = Z$_{\tau-1}$
tail$_{\tau+1}$ zeros$_{\tau}$ = map S nats$_{?}$
\end{Verbatim}
\caption{A Stream of all the natural numbers.}
\label{fig:nats}
\end{figure}

When analysing \texttt{map} we introduce a new time parameter for each coinductive input the function takes. This is not used for checking \texttt{map} but is used for callers of map to see how many observation \texttt{map} performs on a given input. This means that instead of just checking the productivity of \texttt{map} we also create a \textit{specification} for \texttt{map} that other functions can then use. The caller can then judge if the call would be productive or not.

\begin{figure}
\begin{Verbatim}[commandchars=\\\{\},codes={\catcode`$=3\catcode`_=8}]
map$_{\phi, \upsilon}$ : (a -> b) -> Stream a -> Stream b
head$_{\phi+1}$ (map f s)$_{\phi}$ = f (head$_{\upsilon+1}$ s$_{\upsilon}$)
tail$_{\phi+1}$ (map f s)$_{\phi}$ = map f (tail$_{\upsilon+1}$ s$_{\upsilon}$)
\end{Verbatim}
\caption{The map function for Streams.}
\label{fig:map}
\end{figure}

Making the specification is fairly simple. We just have to find the worst case for every input. In the case of \texttt{map} the worst case for time parameter $\upsilon$ is $\upsilon+1$. This tells us that \texttt{map} performs at most one observation on the input. This, however, does not complete the specification. Since we are interested in knowing how many observations are safe to do, and \texttt{map} only observes on its input under its own observations, this allows us to do another observation.\todo{Does this need to be more explicit?} Using the day analogy again this roughly means that tomorrow \texttt{map} will perform the observations of tomorrow. This gives us $\upsilon+1-1$ or just $\upsilon$, which means that the specification for \texttt{map} does not perform any additional observations on its input. This means we can now complete the annotation of \texttt{nats} in Figure~\ref{fig:natsComplete}. Since the specification of \texttt{map} tells us that the input remains unchanged by \texttt{map} we know that the time measure of the call to \texttt{map} with \texttt{nats} as input is $\tau$. We can therefore conclude that \texttt{nats} is productive as $\tau+1\,\textgreater\,\tau$.

\begin{figure}
\begin{Verbatim}[commandchars=\\\{\},codes={\catcode`$=3\catcode`_=8}]
nats$_{\tau}$ : Stream Nat
head$_{\tau+1}$ zeros$_{\tau}$ = Z$_{\tau-1}$
tail$_{\tau+1}$ zeros$_{\tau}$ = map S nats$_{\tau}$
\end{Verbatim}
\caption{A Stream of all the natural numbers with full time annotations.}
\label{fig:natsComplete}
\end{figure}

\subsubsection{Mutual Recursion}
For checking productivity of mutually recursive coinductive definitions we assume that we know what definitions refer to each other. Similar to the previous example we then make a specification for the function that can then be used to check the productivity of the callers. Instead of this specification saying what happens to an input parameter it says what happens to a specific call. Looking at two mutually recursive definitions \texttt{f} and \texttt{g}. For \texttt{f} we define specification $\tau_{g}$ saying what happens to the time parameter $\tau$ in \texttt{g} and vice versa we define $\upsilon_{f}$ for \texttt{g}. For the \texttt{tail} case in \texttt{f} to be productive we need that $\tau+1\,\textgreater\,\tau_{g}$ and for \texttt{g} we need $\upsilon+1\,\textgreater\,\upsilon_{f}+1$ to hold.

\begin{figure}
\begin{Verbatim}[commandchars=\\\{\},codes={\catcode`$=3\catcode`_=8}]
f$_{\tau}$ : Stream Nat
head$_{\tau+1}$ f$_{\tau}$ = Z$_{\tau-1}$
tail$_{\tau+1}$ f$_{\tau}$ = g$_{\tau_{g}}$

g$_{\upsilon}$ : Stream Nat
head$_{\upsilon+1}$ g$_{\upsilon}$ = Z$_{\upsilon-1}$
tail$_{\upsilon+1}$ g$_{\upsilon}$ = tail$_{\upsilon_{f}+1}$ f$_{\upsilon_{f}}$
\end{Verbatim}
\caption{A simple mutual recursive coinductive definitions.}
\label{fig:mutRec1}
\end{figure}

Creating these specifications again is simple, and inferable. To find $\upsilon_{f}$ we look at the references to \texttt{g} in \texttt{f}. We see that there is one observation on the left hand side and none on the right hand side. For the same reason as with \texttt{map} we subtract one for the left hand ones and add one for the right hand side ones. This means that $\upsilon_{f} = \upsilon - 1$. Using the same approach for finding $\tau_{g}$ we get that $\tau_{g} = \tau$. We can now substitute these values into the inequalities from earlier and get that $\tau+1\,\textgreater\,\tau$ and $\upsilon+1\,\textgreater\,\upsilon - 1$. Therefore \texttt{f} and \texttt{g} are productive.

\subsubsection{Known Limitations}
There are a few limitations that we have discovered. In this section we will present the known limitations and discuss how to possibly solve them.

One limitation is if an observation refers to something that is trivially productive, but under more observations. Consider the example in Figure~\ref{fig:forwardRef}. This definition is productive, but will be discarded by our approach as it does not hold that $H+1\,\textgreater\,H+2$. This can be solved by checking such references directly. In this case since \texttt{head tail f} is directly defined we can say that \texttt{head f} is productive if \texttt{head tail f} is productive.

\begin{figure}
\begin{Verbatim}[commandchars=\\\{\},codes={\catcode`$=3\catcode`_=8}]
h$_{H}$ : Stream Nat
head$_{H+1}$ h$_{H}$         \,= head$_{H+2}$ tail$_{H+1}$ h$_{H}$
head$_{H+2}$ tail$_{H+1}$ h$_{H}$ = Z$_{H-1}$
tail$_{H+2}$ tail$_{H+1}$ h$_{H}$ = tail$_{H+1}$ h$_{H}$
\end{Verbatim}
\caption{An example of forward referencing.}
\label{fig:forwardRef}
\end{figure}

Another limitation is functions returning arbitrary functions. In Figure~\ref{fig:funList} we cannot give an accurate time measure to \texttt{(funList !! n) i} because we do not know anything about what \texttt{funList !! n} does to \texttt{i}. A way of attempting to solve this could be to try and estimate a worst case scenario for \texttt{funList}. This, however, involves a lot of unfolding risking making the productivity checker very slow, while still not being a very accurate analysis. We have not been able to find a desirable solution to this problem.

\begin{figure}
\begin{Verbatim}[commandchars=\\\{\},codes={\catcode`$=3\catcode`_=8}]
i$_{I}$ : Nat -> Stream Nat
head$_{I+1}$ (i n)$_{I}$ = Z$_{I-1}$
tail$_{I+1}$ (i n)$_{I}$ = (funList !! n) i$_{?}$

funList : List (Stream a -> Stream a)
funList = [id, id, id]
\end{Verbatim}
\caption{}
\label{fig:funList}
\end{figure}