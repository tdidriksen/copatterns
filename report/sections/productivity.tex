%!TEX root = ../main.tex
\section{Productivity of Cofunctions}
\label{sec:productivity}

Before delving into a description of our proposal for a productivity checking algorithm for definitions with copatterns, we will discuss the properties of other approaches, and why they may or may not be desirable.

\subsection{Lazy Evaluation: The Haskell Approach}
It may be compelling to think that we can simply model coinductive data in the same way as it is done in Haskell, by evaluating all expressions lazily. Then any productivity algorithm would work for Haskell as well, making the result more generally applicable. The lazy evaluation approach, however, is insufficient in several aspects.

First and foremost, if we insist on making no distiction between inductive and coinductive data in our type system (Haskell makes no distinction), subject reduction is lost in a dependently typed system\,\citep{Abel13Copatterns}. The problem lies in the fact that coinductive data is modeled as the \emph{construction} of infinite trees, making dependent pattern matching possible on codata. Consider the type \texttt{U} in Figure~\ref{fig:subject_reduction_problem}, with inhabitant \texttt{u}. Even though it should not hold, the equality \texttt{u = C u} in the type of \texttt{eqU} holds when we allow dependent pattern matching on \texttt{x} in \texttt{eq}, since it allows the type system to reduce \texttt{x} to \texttt{C y}. In this case the type of \texttt{refl} in \texttt{eq} becomes \texttt{C y = force (C y)}, which should hold. However, if we replace the right-hand side of \texttt{eqU} with \texttt{refl}, a type error occurs since \texttt{u} is not equal to \texttt{C u}. Consequently, the type of \texttt{eq} changes when we do dependent pattern matching on its input, \texttt{x}, which means that subject reduction is lost. This problem is explained in greater detail by Abel et al.\,\citep{Abel13Copatterns}, and is also discussed in a correspondence initiated by Danielsson on the Agda mailing list\,\citep{OuryCounterexample}. Naturally, we want to preserve subject reduction in the Idris type system. When we are able to distinguish between inductive and coinductive data, one solution to this problem is to treat coinductive data lazily, making examples like the one in Figure~\ref{fig:subject_reduction_problem} ill-typed. This approach, which is also implemented in Idris, is discussed in Section~\ref{sec:copatterns_in_idris}.

\begin{figure}
\begin{alltt}
data U : Type where         -- No distinction between data and codata
  C : U -> U

fix : (a -> a) -> a
fix f = f (fix f)

u : U
u = fix u

force : U -> U
force x = case x of
            C y => C y

eq : (x : U) -> x = force x
eq x = case x of
         C y => refl

eqU : u = C u
eqU = eq u
\end{alltt}
\caption{Oury's counterexample\,\citep{OuryCounterexample} in a dependently typed language with Haskell-like syntax. Here, \texttt{=} denotes propositional equality, with the sole constructor \texttt{refl}. Dependent pattern matching happens with \texttt{case} expressions.}
\label{fig:subject_reduction_problem}
\end{figure}

Another argument against the Haskell approach is that the core type theory underlying Idris is believed to have the Church-Rosser property\,\citep{BradyIdrisImpl13}, meaning that distinct reduction strategies lead to the same normal form. Furthermore, the total part of Idris is strongly normalizing, such that it enjoys the \emph{strong} Church-Rosser property\,\citep{Turner04totalfunctional}. This means that not only will every reduction strategy leading to a normal form lead to the same normal form, but \emph{any} reduction strategy must lead to a normal form. In Haskell, many definitions will lead to a normal form under lazy evaluation, while eager evaluation would lead to infinite recursion. Therefore, exploiting lazy evaluation in the total part of Idris would mean that it would no longer have the strong Church-Rosser property.

\subsection{Productivity Checking with Sized Types}
As shown by Abel and Pientka\,\citep{Abel13Wellfounded}, verifying the productivity of coinductive definitions with copatterns is indeed possible using sized types. Thus, the main argument against using sized types is one of usability, from the point of view of the Idris user. Thibodeau\,\citep{Thibodeau11} emphasizes that size annotations generally make the code harder to read and seem unnecessary (see Section~\ref{sec:related_work}). In our own experience, size annotations place quite a burden of bookkeeping upon the user, as they have a tendency to become a tool that is used to satisfy the productivity checker, instead of leading to clearer type specifications. Because totality is optional in Idris for definitions that to not appear in types (as opposed to in Agda, for instance), this means that in a practical programming setting, some users would most likely be inclined to switch off productivity checking for coinductive definitions, or refrain from using them entirely.

This argument is only valid as long as we have no way to fully reconstruct all size annotations. Once we do (if ever), the burden of sized types can be placed entirely upon the compiler. At such point, totality checking with sized types would seem like a practical approach.

\subsection{Guarded Corecursion}
\label{sec:guarded_cored}
In its original form presented by Coquand\,\citep{Coquand94} and Gim\'{e}nez\,\citep{Gimenez95}, the guardedness condition is generally too restrictive, as discussed in Section~\ref{sec:related_work}. The work done on the ESFP system by Telford and Turner\,\citep{Telford97ensuringstreams,Telford98ensuringthe} makes the guardedness criterion more generally applicable, even though some problems still remain, such as handling indirect application to corecursive functions. An example of this problem is the \texttt{g} function in Figure~\ref{fig:TelfordTurnerProblems}, the guardedness of which cannot be determined due to the indirect call to another function. In this case, this other function is the identity function, but the point is that the result of \texttt{(fst~funPair)} could have been any function. The function \texttt{f} is not considered guarded within their system because forward references may not be made to the rest of the process, i.e. the head of \texttt{f} cannot refer to the tail of \texttt{f}.

\begin{figure}
\begin{alltt}
-- f = 1 :: 1 :: \ldots                            -- g = 1 :: 1 :: \ldots
f = (head (tail f)) :: (1 :: f)                 g = 1 :: ((fst funPair) g)
                                                where funPair = (id, id)
\end{alltt}
\caption{Two types of corecursive functions not deemed productive by the extended guardedness criterion by Telford and Turner\,\citep[Section~6.3]{Telford98ensuringthe}. Here, \texttt{::} is the cons operator for a hypothetical definition of streams without copatterns, and \texttt{id} is the identity function. The functions \texttt{head} and \texttt{tail} are defined by pattern matching.} 
\label{fig:TelfordTurnerProblems}
\end{figure}

Abel\,\citep{Abel99terminationchecking} argues that termination analysis should abstract from the syntactic structure of the program being analyzed to avoid making the outcome of the analysis susceptible to small changes in program structure. While this is true, non-syntactic checks can require the user to pay more attention to the productivity checker, which might not always be advantageous. 

\paragraph{}
What we ultimately seek is a productivity checking algorithm for definitions with copatterns that covers as many realistic cases as possible, without burdening the user too much. In the following, we will provide a detailed description of our proposal.
%An explanation of our proposal for productivity checking in collaboration with size-change termination.
% Why not guardedness?
% Why not sized types? 
% Why not the haskell way?
\subsection{Proposed Solution}
The basic idea of our proposal is \textit{time measurement}. We wish to always know how many observations we can safely make on a given recursive reference. To do this we use time as an analogy. When defining what happens at a given time we can only refer to what has happened earlier. Each observation is defined by what happens one time unit later. We will exemplify this using days. To know what happens on any given day we can only refer to what has happened earlier than that day. If we translate the days to numerical numbers, where later days are given higher numbers, we can express this idea of only being able to reference back in time with an inequality.

If a given function \texttt{f} ``is defined'' on day $\tau$, then the first observation on \texttt{f} is defined on day $\tau+1$, the second observation on day $\tau+2$ etc. If we wish to define what is defined on any given day $\tau+n$ we can only refer to observations made earlier, on an earlier day $\phi$. We know that $\phi$ is an earlier day if $\tau+n\, \textgreater \,\phi$ holds. Any definition that comply with this is said to be productive.

In Figure~\ref{fig:zeros} we decorate a definition \texttt{zeros} with these time measures. Since \texttt{zeros} is the starting point we give that a base measure $\tau$. Both the \texttt{head} and the \texttt{tail} observations are defined one day later than \texttt{zeros}, they both get time measure $\tau+1$. To determine if \texttt{zeros} is productive we must examine the right hand sides of the observations. The \texttt{head} observation is simple as there are no corecursive calls. Therefore it gets the time measure $stable$ \todo{possible Krishnaswami ref} as we assume that \texttt{Z} is always known. We use this measure for any reference that is not corecursive. In terms of an numeric value $stable$ can be thought of as negative infinity. In the \texttt{tail} observation we have a corecursive reference to \texttt{zeros}. This gets the time measure $\tau$ which is the time of \texttt{zeros}. To determine the productivity we must ensure that observations are only defined by what has already happened. Looking at this as an inequality we can see that \texttt{zeros} is productive because in the \texttt{head} case $\tau+1\,\textgreater\,stable$ and in the \texttt{tail} $\tau+1\,\textgreater\,\tau$. Note that all these annotations can be inferred.

\begin{figure}
\begin{tabular}{l c}

\begin{minipage}{3in}
\begin{Verbatim}[commandchars=\\\{\},codes={\catcode`$=3\catcode`_=8}]
zeros : Stream Nat
head zeros = Z
tail zeros = zeros
\end{Verbatim}
\end{minipage} &
\begin{minipage}{3in}
\begin{Verbatim}[commandchars=\\\{\},codes={\catcode`$=3\catcode`_=8}]
zeros$_{\tau}$ : Stream Nat
head$_{\tau+1}$ zeros$_{\tau}$ = Z$_{stable}$
tail$_{\tau+1}$ zeros$_{\tau}$ = zeros$_{\tau}$
\end{Verbatim}
\end{minipage}

\end{tabular}
\caption{An infinite stream of zeros. To the left is the implementation, and to the right is the implementation decorated with time measures.}
\label{fig:zeros}
\end{figure}

In Figure~\ref{fig:zerosprime} we see a diverging implementation of \texttt{zeros} called \texttt{zeros'}. To establish that this is not productive we decorate the program in the same fashion as above, but this time the recursive reference in the \texttt{tail} case has time measure $\tau+1$ rather than $\tau$ because we make an observation on the right-hand side of \texttt{zeros}. For this to be productive, $\tau+1\,\textgreater\,\tau+1$ must hold which it clearly does not. Therefore we can say that \texttt{zeros'} is not productive.

\begin{figure}
\begin{tabular}{l c}

\begin{minipage}{3in}
\begin{Verbatim}[commandchars=\\\{\},codes={\catcode`$=3\catcode`_=8}]
zeros' : Stream Nat
head zeros' = Z
tail zeros' = tail zeros'
\end{Verbatim}
\end{minipage} &
\begin{minipage}{3in}
\begin{Verbatim}[commandchars=\\\{\},codes={\catcode`$=3\catcode`_=8}]
zeros'$_{\tau}$ : Stream Nat
head$_{\tau+1}$ zeros'$_{\tau}$ = Z$_{stable}$
tail$_{\tau+1}$ zeros'$_{\tau}$ = tail$_{\tau+1}$ zeros'$_{\tau}$
\end{Verbatim}
\end{minipage}

\end{tabular}
\caption{A non-productive implementation of \texttt{zeros}.}
\label{fig:zerosprime}
\end{figure}

\subsubsection{Coinductive Definitions with Coinductive Parameters}

Sometimes we want to write definitions that depend on other functions with coinductive parameters. When decorating \texttt{nats} in Figure~\ref{fig:nats_productivity} we can not give the right hand side of the \texttt{tail} observation an accurate time measure as we don't know what \texttt{map} does to \texttt{nats}. We must first analyse \texttt{map} to see the effect it has on its parameters.

\begin{figure}
\begin{Verbatim}[commandchars=\\\{\},codes={\catcode`$=3\catcode`_=8}]
nats$_{\tau}$ : Stream Nat
head$_{\tau+1}$ nats$_{\tau}$ = Z$_{stable}$
tail$_{\tau+1}$ nats$_{\tau}$ = (map S nats)$_{?}$
\end{Verbatim}
\caption{A \texttt{Stream} of all the natural numbers, annotated with time measures.}
\label{fig:nats_productivity}
\end{figure}

When analysing \texttt{map} we introduce a new time measure for each coinductive input the function takes. This measure is not used for checking the productivity \texttt{map} but is used for callers of \texttt{map} to see how many observations \texttt{map} performs on a given input. This means that in addition to checking the productivity of \texttt{map} we also create a \textit{specification} for \texttt{map} that other functions can then use, to determine whether the call to \texttt{map} would be productive or not.

\begin{figure}
\begin{Verbatim}[commandchars=\\\{\},codes={\catcode`$=3\catcode`_=8}]
map$_{\phi, \upsilon}$ : (a -> b) -> Stream a -> Stream b
head$_{\phi+1}$ (map f s)$_{\phi}$ = f (head$_{\upsilon+1}$ s$_{\upsilon}$)$_{stable}$
tail$_{\phi+1}$ (map f s)$_{\phi}$ = map f (tail$_{\upsilon+1}$ s$_{\upsilon}$)$_{\phi}$
\end{Verbatim}
\caption{The map function for Streams.}
\label{fig:map}
\end{figure}

Making the specification is fairly simple. Since we are interested in knowing how many new observations \texttt{map} possibly makes on an input we are looking for the worst case for every input. In the case of \texttt{map} the worst case for time measure $\upsilon$ is $\upsilon+1$, which tells us that \texttt{map} performs at most one observation on the input. The specification, however, is not done. While it is true that \texttt{map} does make one observation on its input, it does not do so until \texttt{map} itself is observed. This means that any reference to \texttt{map} would need one more observation on its left-hand side to invoke the observations \texttt{map} makes. So even though \texttt{map} adds one observation, it does so one day later. This delay of the observation counteracts the observation map makes. We model this counteraction by subtracting one from the time measure given to $\upsilon$ in the specification.

This gives us $\upsilon+1-1$ or just $\upsilon$, which means that the specification for \texttt{map} does not perform any additional observations on its input. This means we can now complete the annotation of \texttt{nats} from Figure~\ref{fig:nats_productivity}. Since the specification of \texttt{map} tells us that any input remains unchanged, we know that the time measure of the call to \texttt{map} with \texttt{nats} as input in Figure~\ref{fig:natsComplete} is $\tau$. We can therefore conclude that \texttt{nats} is productive, as $\tau+1\,\textgreater\,\tau$.

\begin{figure}
\begin{Verbatim}[commandchars=\\\{\},codes={\catcode`$=3\catcode`_=8}]
nats$_{\tau}$ : Stream Nat
head$_{\tau+1}$ nats$_{\tau}$ = Z$_{stable}$
tail$_{\tau+1}$ nats$_{\tau}$ = (map S nats$_{\tau})$
\end{Verbatim}
\caption{A \texttt{Stream} of all the natural numbers with full time annotations.}
\label{fig:natsComplete}
\end{figure}

\subsubsection{Mutual Recursion}
For checking productivity of mutually recursive coinductive definitions, we assume that we know which definitions refer to each other. Similar to the previous example we then make a specification for each function that can then be used to check the productivity of the callers. Instead of this specification saying what happens to the input, it says what happens to a specific call. Looking at two mutually recursive definitions \texttt{f} and \texttt{g} in Figure~\ref{fig:mutRec1}. For \texttt{f} we define specification $\tau_{g}$ saying what happens to the time measure $\tau$ in \texttt{g} and vice versa we define $\upsilon_{f}$ for \texttt{g}. For the \texttt{tail} case in \texttt{f} to be productive we need that $\tau+1\,\textgreater\,\tau_{g}$ and for \texttt{g} we need $\upsilon+1\,\textgreater\,\upsilon_{f}+1$ to hold.

\begin{figure}
\begin{Verbatim}[commandchars=\\\{\},codes={\catcode`$=3\catcode`_=8}]
f$_{\tau,\upsilon_{f}}$ : Stream Nat
head$_{\tau+1}$ f$_{\tau}$ = Z$_{stable}$
tail$_{\tau+1}$ f$_{\tau}$ = g$_{\tau_{g}}$

g$_{\upsilon,\tau_{g}}$  : Stream Nat
head$_{\upsilon+1}$ g$_{\upsilon}$ = Z$_{stable}$
tail$_{\upsilon+1}$ g$_{\upsilon}$ = tail$_{\upsilon_{f}+1}$ f$_{\upsilon_{f}}$
\end{Verbatim}
\caption{Two mutually corecursive definitions.}
\label{fig:mutRec1}
\end{figure}

Creating these specifications again is simple, and inferable. To find $\upsilon_{f}$ we look at the references to \texttt{g} in \texttt{f}. We see that there is one observation on the left hand side and none on the right hand side. For the same reason as with \texttt{map} we subtract one for the left hand observations and add one for the right hand side observations. This means that $\upsilon_{f} = \upsilon - 1$. Using the same approach for finding $\tau_{g}$ we get that $\tau_{g} = \tau$. We can now substitute these values into the inequalities from earlier and get that $\tau+1\,\textgreater\,\tau$ and $\upsilon+1\,\textgreater\,\upsilon - 1$. Therefore \texttt{f} and \texttt{g} are productive.

\subsubsection{Known Limitations}
We have discovered a few limitations to this approach. In this section we will present them and discuss possible solutions.

Looking at the problems discussed in Section~\ref{sec:guarded_cored} our approach does not solve these directly. The first is if an observation refers to something that is trivially productive, but under more observations. Consider the example in Figure~\ref{fig:forwardRef}. This definition is productive, but will be discarded by our approach as it does not hold that $\tau+1\,\textgreater\,\tau+2$. It is, however, easily solvable due to our use of copatterns. Since \texttt{head tail h} is directly defined we can say that \texttt{head h} is productive if \texttt{head tail h} is productive.

\begin{figure}
\begin{Verbatim}[commandchars=\\\{\},codes={\catcode`$=3\catcode`_=8}]
h$_{\tau}$ : Stream Nat
head$_{\tau+1}$ h$_{\tau}$         = head$_{\tau+2}$ tail$_{\tau+1}$ h$_{\tau}$
head$_{\tau+2}$ tail$_{\tau+1}$ h$_{\tau}$ = Z$_{stable}$
tail$_{\tau+2}$ tail$_{\tau+1}$ h$_{\tau}$ = tail$_{\tau+1}$ h$_{\tau}$
\end{Verbatim}
\caption{An example ...}
\label{fig:forwardRef}
\end{figure}

Another limitation is functions returning arbitrary functions. In Figure~\ref{fig:funList} we cannot give an accurate time measure to \texttt{(funPair) i} because we do not know anything about what \texttt{fst funPair} does to \texttt{i}. Attempting to solve this could be done by estimating a worst case scenario for \texttt{funPair}. This, however, involves a lot of unfolding risking making the productivity checker very slow, while still not providing a very accurate analysis. We have not been able to find a desirable solution to this problem.

\begin{figure}
\begin{Verbatim}[commandchars=\\\{\},codes={\catcode`$=3\catcode`_=8}]
i$_{\tau}$ : Stream Nat
head$_{\tau+1}$ (i)$_{\tau}$ = Z$_{\tau-1}$
tail$_{\tau+1}$ (i)$_{\tau}$ = (fst funPair) i$_{?}$

funPair : (Stream a, Stream a)
funPair = (id, id)
\end{Verbatim}
\caption{}
\label{fig:funList}
\end{figure}