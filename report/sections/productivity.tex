%!TEX root = ../main.tex
\section{Productivity of Cofunctions}
\label{sec:productivity}

Before delving into a description of our proposal for a productivity checking algorithm for definitions with copatterns, we will discuss the properties of other approaches, and why they may or may not be desirable.

\subsection{Lazy Evaluation: The Haskell Approach}
It may be compelling to think that we can simply model coinductive data in the same way as it is done in Haskell, by evaluating all expressions lazily. Then any productivity algorithm would work for Haskell as well, making the result more generally applicable. The lazy evaluation approach, however, is insufficient in several aspects.

First and foremost, if we insist on making no distiction between inductive and coinductive data in our type system (Haskell makes no distinction), subject reduction is lost in a dependently typed system\,\citep{Abel13Copatterns}. The problem lies in the fact that coinductive data is modeled as the \emph{construction} of infinite trees, making dependent pattern matching possible on codata, rather than in terms of its \emph{destructors}. Consider the type \texttt{U} in Figure~\ref{fig:subject_reduction_problem}, with inhabitant \texttt{u}. The equality \texttt{u = C u} in the type of \texttt{eqU} holds when we allow dependent pattern matching on \texttt{x} in \texttt{eq}, since it allows the type system to reduce \texttt{x} to \texttt{C y}. In this case the type of \texttt{refl} in \texttt{eq} becomes \texttt{C y = force (C y)}, which should hold. However, if we replace the right-hand side of \texttt{eq} with \texttt{refl}, a type error occurs since \texttt{u} is not equal to \texttt{C u} in \texttt{eqU}, because the resulting type of this right-hand side would be \texttt{refl : u = force u}, which should not hold. Consequently, the type of \texttt{eq} changes when we do dependent pattern matching on its input, \texttt{x}, which means that subject reduction is lost. This problem is explained in greater detail by Abel et al.\,\citep{Abel13Copatterns}, and is also discussed in a correspondence initiated by Danielsson on the Agda mailing list\,\citep{OuryCounterexample}. Naturally, we want to preserve subject reduction in the Idris type system. When we are able to distinguish between inductive and coinductive data, one solution to this problem is to treat coinductive data lazily, making examples like the one in Figure~\ref{fig:subject_reduction_problem} ill-typed. This approach, which is also implemented in Idris, is discussed in Section~\ref{sec:copatterns_in_idris}.

\begin{figure}
\begin{alltt}
data U : Type where         -- No distinction between data and codata
  C : U -> U

fix : (a -> a) -> a
fix f = f (fix f)

u : U
u = fix u

force : U -> U
force x = case x of
            C y => C y

eq : (x : U) -> x = force x
eq x = case x of
         C y => refl

eqU : u = C u
eqU = eq u
\end{alltt}
\caption{Oury's counterexample\,\citep{OuryCounterexample} in a dependently typed language with Haskell-like syntax. Here, \texttt{=} denotes propositional equality, with the sole constructor \texttt{refl}. Dependent pattern matching happens with \texttt{case} expressions.}
\label{fig:subject_reduction_problem}
\end{figure}

Another argument against the Haskell approach is that the core type theory underlying Idris is believed to have the Church-Rosser property, meaning that distinct reduction strategies lead to the same normal form\,\citep{BradyIdrisImpl13}. Furthermore, the total part of Idris is strongly normalizing, such that it enjoys the \emph{strong} Church-Rosser property\,\citep{Turner04totalfunctional}. This means that not only will every reduction strategy leading to a normal form lead to the same normal form, but \emph{any} reduction strategy must lead to a normal form. In Haskell, many definitions will lead to a normal form under lazy evaluation, while eager evaluation would lead to infinite recursion. Therefore, exploiting lazy evaluation in the total part of Idris would mean that it would no longer have the strong Church-Rosser property.

\subsection{Productivity Checking with Sized Types}
As shown by Abel and Pientka\,\citep{Abel13Wellfounded}, verifying the productivity of coinductive definitions with copatterns is indeed possible using sized types. Thus, the main argument against using sized types is one of usability, from the point of view of the Idris user. Thibodeau\,\citep{Thibodeau11} emphasizes that size annotations generally make the code harder to read and seem unnecessary (see Section~\ref{sec:related_work}). In our own experience, size annotations place quite a burden of bookkeeping upon the user, as they have a tendency to become a tool that is used to satisfy the productivity checker, instead of leading to clearer type specifications.

Because totality is optional in Idris for definitions that to not appear in types (as opposed to in Agda, for instance), this means that in a practical programming setting, some users would most likely be inclined to switch off productivity checking for coinductive definitions, or refrain from using them entirely.

This argument is only valid as long as we have no way to fully reconstruct all size annotations. Once we do (if ever), the burden of sized types can be placed entirely upon the compiler. At such point, termination checking with sized types would seem like a useful approach.

\subsection{Guarded Corecursion}
In its original form presented by Coquand\,\citep{Coquand94} and Gim\'{e}nez\,\citep{Gimenez95}, the guardedness condition is generally too restrictive, as discussed in Section~\ref{sec:related_work}. The work done on the ESFP system by Telford and Turner\,\citep{Telford97ensuringstreams,Telford98ensuringthe} makes the guardedness criterion more generally applicable, even though some problems still remain, such as handling indirect application to corecursive functions. An example of this problem is the \texttt{g} function in Figure~\ref{fig:TelfordTurnerProblems}, whose guardedness cannot be determined due to the indirect call to another function. In this case, this other function is the identity function, but the point is that the result of \texttt{(fst funPair)} could have been any function. The function \texttt{f} is not considered guarded within their system because forward references may not be made to the rest of the process, i.e. the head of \texttt{f} cannot refer to the tail of \texttt{f}.

\begin{figure}
\begin{alltt}
-- f = 1 :: 1 :: \ldots                            -- g = 1 :: 1 :: \ldots
f = (head (tail f)) :: (1 :: f)                 g = 1 :: ((fst funPair) g)
                                                where funPair = (id, id)
\end{alltt}
\caption{Two types of functions not deemed productive by the extended guardedness criterion by Telford and Turner\,\citep[Section~6.3]{Telford98ensuringthe}. Here, \texttt{::} is the cons operator for a hypothetical definition of streams without copatterns, and \texttt{id} is the identity function. The functions \texttt{head} and \texttt{tail} are defined by pattern matching.} 
\label{fig:TelfordTurnerProblems}
\end{figure}

Abel\,\citep{Abel99terminationchecking} argues that termination analysis should abstract from the syntactic structure of the program being analyzed to avoid making the outcome of the analysis susceptible to small changes in program structure. While this is true, non-syntactic checks can require the user to pay more attention to the productivity checker, which might not always be advantageous. 

\paragraph{}
What we ultimately seek is a productivity checking algorithm for definitions with copatterns that covers as many realistic cases as possible, without burdening the user too much. In the following, we will provide a detailed description of our proposal.
%An explanation of our proposal for productivity checking in collaboration with size-change termination.
% Why not guardedness?
% Why not sized types? 
% Why not the haskell way?
\subsection{Proposed Solution}
The basic idea of our proposal is \textit{time measurement}. We wish to always know how many observations we can safely make on a given recursive reference. To do this we use time as an analogy. To define what happens at a given time we can only refer to what has happened earlier. Each observation in a copattern says what happens one time unit later. For this we will exemplify using days. If a given function \texttt{f} ``happens'' on day $\tau$, then the first observation on \texttt{f} happens on day $\tau+1$, the second observation on day $\tau+2$ etc. If we wish to define what happens on any given day $\tau+n$ we can only refer to things happening earlier, on day $D$ where $\tau+n\, \textgreater \,D$. Any definition that comply with this is said to be productive.

In Figure~\ref{fig:zeros} we decorate a definition \texttt{zeros} with these time measures. Since both the \texttt{head} and the \texttt{tail} cases are one observation deep, they both get time measure $\tau+1$. Now we must look on the right hand sides of these observations. The \texttt{head} observation is simple as there are no recursive calls. This gets the time measure $stable$ \todo{possible Krishnaswami ref} as we assume \texttt{Z} is always known. In the \texttt{tail} observation we have a recursive reference to \texttt{zeros}. This gets the time measure $\tau$ which is the time of \texttt{zeros}. We can say that \texttt{zeros} is productive because in the \texttt{head} case $\tau+1\,\textgreater\,stable$ and in the \texttt{tail} $\tau+1\,\textgreater\,\tau$. Note that all these annotations can be inferred, and does not need the user to specify them.

\begin{figure}
\begin{tabular}{l c}

\begin{minipage}{3in}
\begin{Verbatim}[commandchars=\\\{\},codes={\catcode`$=3\catcode`_=8}]
zeros : Stream Nat
head zeros = Z
tail zeros = zeros
\end{Verbatim}
\end{minipage} &
\begin{minipage}{3in}
\begin{Verbatim}[commandchars=\\\{\},codes={\catcode`$=3\catcode`_=8}]
zeros$_{\tau}$ : Stream Nat
head$_{\tau+1}$ zeros$_{\tau}$ = Z$_{stable}$
tail$_{\tau+1}$ zeros$_{\tau}$ = zeros$_{\tau}$
\end{Verbatim}
\end{minipage}

\end{tabular}
\caption{An infinite stream of zeros. To the left is the implementation, and to the right is the implementation decorated with depth measures.}
\label{fig:zeros}
\end{figure}

In Figure~\ref{fig:zerosprime} we see a faulty implementation of \texttt{zeros} called \texttt{zeros'}. This is not a productive implementation. We decorate the program in the same fashion as above, but this time the recursive reference in the \texttt{tail} case has time measure $\tau+1$ rather than $\tau$ because we make a right hand side observation on it. For this to be productive $\tau+1\,\textgreater\,\tau+1$ which it clearly is not. Therefore we can say that \texttt{zeros'} is not productive.

\begin{figure}
\begin{tabular}{l c}

\begin{minipage}{3in}
\begin{Verbatim}[commandchars=\\\{\},codes={\catcode`$=3\catcode`_=8}]
zeros' : Stream Nat
head zeros' = Z
tail zeros' = tail zeros'
\end{Verbatim}
\end{minipage} &
\begin{minipage}{3in}
\begin{Verbatim}[commandchars=\\\{\},codes={\catcode`$=3\catcode`_=8}]
zeros'$_{\tau}$ : Stream Nat
head$_{\tau+1}$ zeros'$_{\tau}$ = Z$_{stable}$
tail$_{\tau+1}$ zeros'$_{\tau}$ = tail$_{\tau+1}$ zeros'$_{\tau}$
\end{Verbatim}
\end{minipage}

\end{tabular}
\caption{A non-productive implementation of zeros.}
\label{fig:zerosprime}
\end{figure}

\subsubsection{Coinductive Definitions with Coinductive Parameters}

Sometimes we want to write definitions that depend on other functions with coinductive parameters. When decorating \texttt{nats} in Figure~\ref{fig:nats_productivity} we can not give the right hand side of the \texttt{tail} observation an accurate time measure as we don't know what \texttt{map} does to \texttt{nats}. We must first analyse \texttt{map} to see the effect it has on it parameters.

\begin{figure}
\begin{Verbatim}[commandchars=\\\{\},codes={\catcode`$=3\catcode`_=8}]
nats$_{\tau}$ : Stream Nat
head$_{\tau+1}$ zeros$_{\tau}$ = Z$_{stable}$
tail$_{\tau+1}$ zeros$_{\tau}$ = map S nats$_{?}$
\end{Verbatim}
\caption{A Stream of all the natural numbers.}
\label{fig:nats_productivity}
\end{figure}

When analysing \texttt{map} we introduce a new time parameter for each coinductive input the function takes. This is not used for checking \texttt{map} but is used for callers of map to see how many observation \texttt{map} performs on a given input. This means that instead of just checking the productivity of \texttt{map} we also create a \textit{specification} for \texttt{map} that other functions can then use. The caller can then judge if the call would be productive or not.

\begin{figure}
\begin{Verbatim}[commandchars=\\\{\},codes={\catcode`$=3\catcode`_=8}]
map$_{\phi, \upsilon}$ : (a -> b) -> Stream a -> Stream b
head$_{\phi+1}$ (map f s)$_{\phi}$ = f (head$_{\upsilon+1}$ s$_{\upsilon}$)
tail$_{\phi+1}$ (map f s)$_{\phi}$ = map f (tail$_{\upsilon+1}$ s$_{\upsilon}$)
\end{Verbatim}
\caption{The map function for Streams.}
\label{fig:map}
\end{figure}

Making the specification is fairly simple. We just have to find the worst case for every input. In the case of \texttt{map} the worst case for time parameter $\upsilon$ is $\upsilon+1$. This tells us that \texttt{map} performs at most one observation on the input. This, however, does not complete the specification. Since we are interested in knowing how many observations are safe to do, and \texttt{map} only observes on its input under its own observations, this allows us to do another observation.\todo{Does this need to be more explicit?} Using the day analogy again this roughly means that tomorrow \texttt{map} will perform the observations of tomorrow. This gives us $\upsilon+1-1$ or just $\upsilon$, which means that the specification for \texttt{map} does not perform any additional observations on its input. This means we can now complete the annotation of \texttt{nats} in Figure~\ref{fig:natsComplete}. Since the specification of \texttt{map} tells us that the input remains unchanged by \texttt{map} we know that the time measure of the call to \texttt{map} with \texttt{nats} as input is $\tau$. We can therefore conclude that \texttt{nats} is productive as $\tau+1\,\textgreater\,\tau$.

\begin{figure}
\begin{Verbatim}[commandchars=\\\{\},codes={\catcode`$=3\catcode`_=8}]
nats$_{\tau}$ : Stream Nat
head$_{\tau+1}$ zeros$_{\tau}$ = Z$_{stable}$
tail$_{\tau+1}$ zeros$_{\tau}$ = map S nats$_{\tau}$
\end{Verbatim}
\caption{A Stream of all the natural numbers with full time annotations.}
\label{fig:natsComplete}
\end{figure}

\subsubsection{Mutual Recursion}
For checking productivity of mutually recursive coinductive definitions we assume that we know what definitions refer to each other. Similar to the previous example we then make a specification for the function that can then be used to check the productivity of the callers. Instead of this specification saying what happens to an input parameter it says what happens to a specific call. Looking at two mutually recursive definitions \texttt{f} and \texttt{g}. For \texttt{f} we define specification $\tau_{g}$ saying what happens to the time parameter $\tau$ in \texttt{g} and vice versa we define $\upsilon_{f}$ for \texttt{g}. For the \texttt{tail} case in \texttt{f} to be productive we need that $\tau+1\,\textgreater\,\tau_{g}$ and for \texttt{g} we need $\upsilon+1\,\textgreater\,\upsilon_{f}+1$ to hold.

\begin{figure}
\begin{Verbatim}[commandchars=\\\{\},codes={\catcode`$=3\catcode`_=8}]
f$_{\tau}$ : Stream Nat
head$_{\tau+1}$ f$_{\tau}$ = Z$_{\tau-1}$
tail$_{\tau+1}$ f$_{\tau}$ = g$_{\tau_{g}}$

g$_{\upsilon}$ : Stream Nat
head$_{\upsilon+1}$ g$_{\upsilon}$ = Z$_{\upsilon-1}$
tail$_{\upsilon+1}$ g$_{\upsilon}$ = tail$_{\upsilon_{f}+1}$ f$_{\upsilon_{f}}$
\end{Verbatim}
\caption{A simple mutual recursive coinductive definitions.}
\label{fig:mutRec1}
\end{figure}

Creating these specifications again is simple, and inferable. To find $\upsilon_{f}$ we look at the references to \texttt{g} in \texttt{f}. We see that there is one observation on the left hand side and none on the right hand side. For the same reason as with \texttt{map} we subtract one for the left hand ones and add one for the right hand side ones. This means that $\upsilon_{f} = \upsilon - 1$. Using the same approach for finding $\tau_{g}$ we get that $\tau_{g} = \tau$. We can now substitute these values into the inequalities from earlier and get that $\tau+1\,\textgreater\,\tau$ and $\upsilon+1\,\textgreater\,\upsilon - 1$. Therefore \texttt{f} and \texttt{g} are productive.

\subsubsection{Known Limitations}
There are a few limitations that we have discovered. In this section we will present the known limitations and discuss how to possibly solve them.

One limitation is if an observation refers to something that is trivially productive, but under more observations. Consider the example in Figure~\ref{fig:forwardRef}. This definition is productive, but will be discarded by our approach as it does not hold that $\tau+1\,\textgreater\,\tau+2$. This can be solved by checking such references directly. In this case since \texttt{head tail f} is directly defined we can say that \texttt{head f} is productive if \texttt{head tail f} is productive.

\begin{figure}
\begin{Verbatim}[commandchars=\\\{\},codes={\catcode`$=3\catcode`_=8}]
h$_{\tau}$ : Stream Nat
head$_{\tau+1}$ h$_{\tau}$         = head$_{\tau+2}$ tail$_{\tau+1}$ h$_{\tau}$
head$_{\tau+2}$ tail$_{\tau+1}$ h$_{\tau}$ = Z$_{stable}$
tail$_{\tau+2}$ tail$_{\tau+1}$ h$_{\tau}$ = tail$_{\tau+1}$ h$_{\tau}$
\end{Verbatim}
\caption{An example of forward referencing.}
\label{fig:forwardRef}
\end{figure}

Another limitation is functions returning arbitrary functions. In Figure~\ref{fig:funList} we cannot give an accurate time measure to \texttt{(funList !! n) i} because we do not know anything about what \texttt{funList !! n} does to \texttt{i}. A way of attempting to solve this could be to try and estimate a worst case scenario for \texttt{funList}. This, however, involves a lot of unfolding risking making the productivity checker very slow, while still not being a very accurate analysis. We have not been able to find a desirable solution to this problem.

\begin{figure}
\begin{Verbatim}[commandchars=\\\{\},codes={\catcode`$=3\catcode`_=8}]
i$_{\tau}$ : Nat -> Stream Nat
head$_{\tau+1}$ (i n)$_{\tau}$ = Z$_{\tau-1}$
tail$_{\tau+1}$ (i n)$_{\tau}$ = (funList !! n) i$_{?}$

funList : List (Stream a -> Stream a)
funList = [id, id, id]
\end{Verbatim}
\caption{}
\label{fig:funList}
\end{figure}